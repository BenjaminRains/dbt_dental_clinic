"""
Configuration settings for the ETL pipeline.
Consolidates database and pipeline configuration logic.

Phase 4 Implementation - Clean Configuration Structure:
- Removed three-section YAML support (source_tables, staging_tables, target_tables)
- Simplified to single 'tables' section structure
- Removed all backward compatibility methods
- Clean, simple implementation only

Phase 5 Implementation - Environment-Aware Configuration:
- Added environment parameter to Settings constructor
- Supports environment-specific variable prefixes (TEST_ for test environment)
- Automatic environment detection from ETL_ENVIRONMENT, ENVIRONMENT, or APP_ENV
- Backward compatible with existing code

Current Status:
- This is the ACTIVE configuration system for new development
- All new code should use Settings class and global settings instance
- Supports environment variables, YAML files, and validation
- Uses simplified tables.yml structure generated by analyze_opendental_schema.py
- Environment-aware configuration with automatic detection

Usage:
- Import: from etl_pipeline.config import settings
- Access: settings.get_database_config('source')
- Table config: settings.get_table_config('patient')
- Validate: settings.validate_configs()

Environment Configuration:
- Production (default): Uses base variable names (OPENDENTAL_SOURCE_HOST)
- Test: Uses TEST_ prefixed variables (TEST_OPENDENTAL_SOURCE_HOST)
- Set ETL_ENVIRONMENT environment variable to control environment
"""
import os
import logging
from typing import Dict, Optional, List
from pathlib import Path
from dotenv import load_dotenv
import yaml

# Use standard logging instead of importing from core
logger = logging.getLogger(__name__)

class Settings:
    """Clean configuration settings manager - no backward compatibility."""
    
    # Environment variable mappings for base database connections
    ENV_MAPPINGS = {
        'source': {
            'host': 'OPENDENTAL_SOURCE_HOST',
            'port': 'OPENDENTAL_SOURCE_PORT',
            'database': 'OPENDENTAL_SOURCE_DB',
            'user': 'OPENDENTAL_SOURCE_USER',
            'password': 'OPENDENTAL_SOURCE_PASSWORD'
        },
        'replication': {
            'host': 'MYSQL_REPLICATION_HOST',
            'port': 'MYSQL_REPLICATION_PORT',
            'database': 'MYSQL_REPLICATION_DB',
            'user': 'MYSQL_REPLICATION_USER',
            'password': 'MYSQL_REPLICATION_PASSWORD'
        },
        'analytics': {
            'host': 'POSTGRES_ANALYTICS_HOST',
            'port': 'POSTGRES_ANALYTICS_PORT',
            'database': 'POSTGRES_ANALYTICS_DB',
            'schema': 'POSTGRES_ANALYTICS_SCHEMA',
            'user': 'POSTGRES_ANALYTICS_USER',
            'password': 'POSTGRES_ANALYTICS_PASSWORD'
        }
        # Test database configurations removed - will use prefixed variables instead
    }
    
    # Mapping from specific connection names to base connection types
    CONNECTION_MAPPINGS = {
        # Specific connection names -> base connection types
        'opendental_source': 'source',
        'opendental_replication': 'replication',
        'opendental_analytics_public': 'analytics',
        'opendental_analytics_raw': 'analytics',
        'opendental_analytics_staging': 'analytics',
        'opendental_analytics_intermediate': 'analytics',
        'opendental_analytics_marts': 'analytics',
        
        # Test database connections (will use prefixed environment variables)
        'test_opendental_source': 'source',
        'test_opendental_replication': 'replication',
        'test_opendental_analytics': 'analytics',
        'test_opendental_analytics_raw': 'analytics',
        'test_opendental_analytics_public': 'analytics',
        'test_opendental_analytics_staging': 'analytics',
        'test_opendental_analytics_intermediate': 'analytics',
        'test_opendental_analytics_marts': 'analytics',
        
        # Legacy names for backward compatibility
        'source': 'source',
        'staging': 'replication',  # staging was actually replication
        'target': 'analytics'      # target was actually analytics
    }
    
    def __init__(self, environment='production'):
        """
        Initialize settings and load configuration.
        
        Args:
            environment: Environment name ('production', 'test')
                        Defaults to 'production' for backward compatibility
        """
        self.environment = environment
        # Set prefix for environment-specific variables (matches .env.template)
        # Only 'test' environment uses prefix, production uses base names
        self.env_prefix = "TEST_" if environment == 'test' else ""
        
        self.load_environment()
        self.pipeline_config = self.load_pipeline_config()
        self.tables_config = self.load_tables_config()
        self._connection_cache = {}
    
    @classmethod
    def load_environment(cls) -> None:
        """Load environment variables from .env files."""
        current_dir = Path(__file__).parent.parent
        
        # Try to load from etl_pipeline/.env first
        etl_env_path = current_dir / '.env'
        if etl_env_path.exists():
            logger.info(f"Loading environment from {etl_env_path}")
            load_dotenv(etl_env_path)
        else:
            # Fall back to parent directory .env
            parent_env_path = current_dir.parent / '.env'
            if parent_env_path.exists():
                logger.info(f"Loading environment from {parent_env_path}")
                load_dotenv(parent_env_path)
            else:
                logger.warning("No .env file found in etl_pipeline or parent directory")
    
    def load_pipeline_config(self) -> Dict:
        """Load pipeline configuration from YAML file."""
        config_dir = Path(__file__).parent
        # Try both .yml and .yaml extensions
        for ext in ['.yml', '.yaml']:
            config_path = config_dir / f"pipeline{ext}"
            if config_path.exists():
                try:
                    with open(config_path, 'r') as f:
                        return yaml.safe_load(f) or {}
                except Exception as e:
                    logger.error(f"Failed to load pipeline config: {e}")
                    return {}
        
        logger.warning(f"Pipeline config file not found in {config_dir}")
        return {}
    
    def load_tables_config(self) -> Dict:
        """Load tables configuration from simplified YAML structure."""
        config_dir = Path(__file__).parent
        # Try both .yml and .yaml extensions
        for ext in ['.yml', '.yaml']:
            config_path = config_dir / f"tables{ext}"
            if config_path.exists():
                try:
                    with open(config_path, 'r') as f:
                        config = yaml.safe_load(f) or {}
                        
                        # Validate the new structure
                        if 'tables' not in config:
                            logger.warning("Tables config missing 'tables' section - using empty config")
                            return {'tables': {}}
                        
                        return config
                except Exception as e:
                    logger.error(f"Failed to load tables config: {e}")
                    return {'tables': {}}
        
        logger.warning(f"Tables config file not found in {config_dir}")
        return {'tables': {}}
    
    def get_database_config(self, db_type: str) -> Dict:
        """Get database configuration for specified type."""
        if db_type in self._connection_cache:
            return self._connection_cache[db_type]
        
        # Map specific connection names to base connection types
        base_db_type = self.CONNECTION_MAPPINGS.get(db_type, db_type)
        
        config = self._get_base_config(base_db_type)
        
        # Merge with any pipeline config overrides
        pipeline_connections = self.pipeline_config.get('connections', {})
        if db_type in pipeline_connections:
            config.update(pipeline_connections[db_type])
        
        # Add default connection parameters
        if base_db_type in ['source', 'replication']:
            # MySQL defaults
            config.setdefault('connect_timeout', 10)
            config.setdefault('read_timeout', 30)
            config.setdefault('write_timeout', 30)
            config.setdefault('charset', 'utf8mb4')
        else:  # analytics (PostgreSQL)
            # PostgreSQL defaults
            config.setdefault('connect_timeout', 10)
            config.setdefault('application_name', 'etl_pipeline')
            
            # Handle schema-specific configurations for analytics
            if db_type in ['opendental_analytics_public', 'opendental_analytics_raw', 
                          'opendental_analytics_staging', 'opendental_analytics_intermediate', 
                          'opendental_analytics_marts']:
                # Extract schema from connection name
                schema_map = {
                    'opendental_analytics_public': 'public',
                    'opendental_analytics_raw': 'raw',
                    'opendental_analytics_staging': 'public_staging',
                    'opendental_analytics_intermediate': 'public_intermediate',
                    'opendental_analytics_marts': 'public_marts'
                }
                config['schema'] = schema_map.get(db_type, 'public')
            elif db_type in ['test_opendental_analytics_raw', 'test_opendental_analytics_public',
                           'test_opendental_analytics_staging', 'test_opendental_analytics_intermediate',
                           'test_opendental_analytics_marts']:
                # Extract schema from test connection name
                schema_map = {
                    'test_opendental_analytics_public': 'public',
                    'test_opendental_analytics_raw': 'raw',
                    'test_opendental_analytics_staging': 'public_staging',
                    'test_opendental_analytics_intermediate': 'public_intermediate',
                    'test_opendental_analytics_marts': 'public_marts'
                }
                config['schema'] = schema_map.get(db_type, 'public')
        
        self._connection_cache[db_type] = config
        return config
    
    def _get_base_config(self, db_type: str) -> Dict:
        """Get base configuration for a database type."""
        if db_type not in self.ENV_MAPPINGS:
            raise ValueError(f"Unknown database type: {db_type}")
        
        env_mapping = self.ENV_MAPPINGS[db_type]
        config = {}
        
        for key, env_var in env_mapping.items():
            # Try environment-specific variable first, then fall back to base name
            prefixed_env_var = f"{self.env_prefix}{env_var}"
            value = os.getenv(prefixed_env_var) or os.getenv(env_var)
            
            if key == 'port' and value:
                try:
                    value = int(value)
                except ValueError:
                    logger.warning(f"Invalid port value for {env_var}: {value}")
                    value = 3306 if db_type in ['source', 'replication'] else 5432
            config[key] = value
        
        return config
    
    def get_connection_string(self, db_type: str) -> str:
        """Get SQLAlchemy connection string for a database type."""
        config = self.get_database_config(db_type)
        
        # Validate required fields
        required_fields = ['host', 'port', 'database', 'user', 'password']
        missing_fields = [field for field in required_fields if not config.get(field)]
        if missing_fields:
            raise ValueError(f"Missing required database config fields for {db_type}: {missing_fields}")
        
        # Map specific connection names to base connection types
        base_db_type = self.CONNECTION_MAPPINGS.get(db_type, db_type)
        
        if base_db_type in ['source', 'replication']:
            # MySQL connection string
            return (
                f"mysql+pymysql://{config['user']}:{config['password']}@"
                f"{config['host']}:{config['port']}/{config['database']}"
                f"?connect_timeout={config.get('connect_timeout', 10)}"
                f"&read_timeout={config.get('read_timeout', 30)}"
                f"&write_timeout={config.get('write_timeout', 30)}"
                f"&charset={config.get('charset', 'utf8mb4')}"
            )
        else:  # analytics (PostgreSQL)
            # PostgreSQL connection string
            conn_str = (
                f"postgresql+psycopg2://{config['user']}:{config['password']}@"
                f"{config['host']}:{config['port']}/{config['database']}"
                f"?connect_timeout={config.get('connect_timeout', 10)}"
                f"&application_name={config.get('application_name', 'etl_pipeline')}"
            )
            if config.get('schema'):
                conn_str += f"&options=-csearch_path%3D{config['schema']}"
            return conn_str
    
    def validate_configs(self) -> bool:
        """Validate that all required configurations are present."""
        missing_vars = []
        empty_vars = []
        
        for db_type, env_mapping in self.ENV_MAPPINGS.items():
            for key, env_var in env_mapping.items():
                if key == 'schema':  # Schema is optional for PostgreSQL
                    continue
                    
                # Check both prefixed and base variable names
                prefixed_env_var = f"{self.env_prefix}{env_var}"
                value = os.getenv(prefixed_env_var) or os.getenv(env_var)
                
                if not value:
                    missing_vars.append(f"{db_type}: {env_var} (tried {prefixed_env_var}, {env_var})")
                elif value.lower() in ['none', 'null', '']:
                    empty_vars.append(f"{db_type}: {env_var} = '{value}'")
        
        if missing_vars or empty_vars:
            logger.error(f"Configuration issues found for environment '{self.environment}':")
            if missing_vars:
                logger.error("Missing variables:")
                for var in missing_vars:
                    logger.error(f"  - {var}")
            if empty_vars:
                logger.error("Empty/invalid variables:")
                for var in empty_vars:
                    logger.error(f"  - {var}")
            return False
        
        return True
    
    def get_pipeline_setting(self, key: str, default=None):
        """Get a pipeline configuration setting.
        
        Args:
            key: The setting key, can use dot notation for nested keys (e.g. 'general.batch_size')
            default: Default value to return if key is not found
            
        Returns:
            The setting value or default if not found
        """
        if not key:
            return default
            
        # Split the key into parts
        parts = key.split('.')
        value = self.pipeline_config
        
        # Traverse the nested dictionary
        for part in parts:
            if not isinstance(value, dict):
                return default
            value = value.get(part)
            if value is None:
                return default
                
        return value
    
    def get_table_config(self, table_name: str) -> Dict:
        """Get configuration for a specific table from simplified structure."""
        tables = self.tables_config.get('tables', {})
        config = tables.get(table_name, {})
        
        if not config:
            logger.warning(f"No configuration found for table {table_name}")
            return self._get_default_table_config()
        
        return config
    
    def _get_default_table_config(self) -> Dict:
        """Get default configuration for tables not in config."""
        return {
            'incremental_column': None,
            'batch_size': 5000,
            'extraction_strategy': 'full_table',
            'table_importance': 'standard',
            'estimated_size_mb': 0,
            'estimated_rows': 0,
            'dependencies': [],
            'is_modeled': False,
            'monitoring': {
                'alert_on_failure': False,
                'max_extraction_time_minutes': 30,
                'data_quality_threshold': 0.95
            }
        }
    
    def list_tables(self) -> List[str]:
        """List all configured tables."""
        return list(self.tables_config.get('tables', {}).keys())
    
    def get_tables_by_importance(self, importance_level: str) -> List[str]:
        """Get tables by importance level."""
        tables = self.tables_config.get('tables', {})
        return [
            table_name for table_name, config in tables.items()
            if config.get('table_importance') == importance_level
        ]
    
    def should_use_incremental(self, table_name: str) -> bool:
        """Check if table should use incremental loading."""
        config = self.get_table_config(table_name)
        return config.get('extraction_strategy') == 'incremental' and config.get('incremental_column') is not None
    
    def get_table_dependencies(self, table_name: str) -> List[str]:
        """Get dependencies for a specific table."""
        config = self.get_table_config(table_name)
        return config.get('dependencies', [])
    
    def get_tables_by_extraction_strategy(self, strategy: str) -> List[str]:
        """Get tables by extraction strategy."""
        tables = self.tables_config.get('tables', {})
        return [
            table_name for table_name, config in tables.items()
            if config.get('extraction_strategy') == strategy
        ]
    
    def get_monitoring_config(self, table_name: str) -> Dict:
        """Get monitoring configuration for a table."""
        config = self.get_table_config(table_name)
        return config.get('monitoring', {})
    
    def should_alert_on_failure(self, table_name: str) -> bool:
        """Check if table should alert on failure."""
        monitoring_config = self.get_monitoring_config(table_name)
        return monitoring_config.get('alert_on_failure', False)
    
    def get_max_extraction_time(self, table_name: str) -> int:
        """Get maximum extraction time for a table in minutes."""
        monitoring_config = self.get_monitoring_config(table_name)
        return monitoring_config.get('max_extraction_time_minutes', 30)
    
    def get_data_quality_threshold(self, table_name: str) -> float:
        """Get data quality threshold for a table."""
        monitoring_config = self.get_monitoring_config(table_name)
        return monitoring_config.get('data_quality_threshold', 0.95)

def get_global_settings():
    """Get global settings instance with environment detection."""
    # Detect environment from various sources
    environment = (
        os.getenv('ETL_ENVIRONMENT') or
        os.getenv('ENVIRONMENT') or
        os.getenv('APP_ENV') or
        'production'  # Default fallback
    )
    
    # Validate environment
    valid_environments = ['production', 'test']
    if environment not in valid_environments:
        logger.warning(f"Invalid environment '{environment}', using 'production'")
        environment = 'production'
    
    logger.info(f"Initializing Settings for environment: {environment}")
    return Settings(environment=environment)

# Create global settings instance
settings = get_global_settings()